{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.\tВыбор начальных условий\n",
    "\n",
    "**a. Выбор набора данных для задачи классификации**\n",
    "\n",
    "Для задачи классификации выбран набор данных **Fashion-MNIST**. Этот выбор обоснован тем, что набор данных представляет собой реальную практическую задачу классификации изображений одежды, что может быть полезно для разработки приложений в области электронной коммерции или автоматизации процессов на складах.\n",
    "\n",
    "**b. Выбор метрик качества**\n",
    "\n",
    "Для оценки качества моделей классификации выбраны следующие метрики:\n",
    "\n",
    "1. **Точность (Accuracy):** доля правильно классифицированных примеров среди всех рассмотренных. Эта метрика подходит для задач с примерно одинаковым количеством примеров в каждом классе.\n",
    "\n",
    "2. **F1-мера:** гармоническое среднее между точностью (precision) и полнотой (recall). Эта метрика полезна, когда классы в данных несбалансированы."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Создание бейзлайна и оценка качества\n",
    "\n",
    "**a/b. Обучение и оценка качество моделей по выбранным метрикам на выбранном наборе данных**\n",
    "\n",
    "Для обучения моделей будем использовать библиотеку PyTorch и torchvision."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Обучаем ViT ===\n",
      "Epoch 1/3: val acc = 0.8186\n",
      "Epoch 2/3: val acc = 0.8560\n",
      "Epoch 3/3: val acc = 0.9003\n",
      "ViT_B_16: Test accuracy: 0.9003, Test Macro F1: 0.9016\n",
      "=== Обучаем ResNet (CNN) ===\n",
      "Epoch 1/3: val acc = 0.8572\n",
      "Epoch 2/3: val acc = 0.8780\n",
      "Epoch 3/3: val acc = 0.8759\n",
      "ResNet: Test accuracy: 0.8759, Test Macro F1: 0.8757\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "from torch.utils.data import Subset\n",
    "import torchvision.transforms as transforms\n",
    "from torchmetrics.classification import MulticlassAccuracy, MulticlassF1Score\n",
    "\n",
    "device = torch.device(\"mps\")\n",
    "BATCH_SIZE = 128\n",
    "NUM_CLASSES = 10\n",
    "EPOCHS = 3\n",
    "LR = 1e-4\n",
    "\n",
    "transform_cnn = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,))\n",
    "])\n",
    "transform_vit = transforms.Compose([\n",
    "    transforms.Resize(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Lambda(lambda x: x.repeat(3, 1, 1)),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "])\n",
    "\n",
    "trainset_cnn = torchvision.datasets.FashionMNIST('data', train=True, download=True, transform=transform_cnn)\n",
    "testset_cnn  = torchvision.datasets.FashionMNIST('data', train=False, download=True, transform=transform_cnn)\n",
    "trainset_vit = torchvision.datasets.FashionMNIST('data', train=True, download=True, transform=transform_vit)\n",
    "testset_vit  = torchvision.datasets.FashionMNIST('data', train=False, download=True, transform=transform_vit)\n",
    "\n",
    "\n",
    "trainloader_cnn = torch.utils.data.DataLoader(trainset_cnn, batch_size=BATCH_SIZE, shuffle=True)\n",
    "testloader_cnn  = torch.utils.data.DataLoader(testset_cnn, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "N_SUBSET = 5000\n",
    "np.random.seed(42)\n",
    "indices_train = np.random.choice(len(trainset_vit), N_SUBSET, replace=False)\n",
    "trainset_vit_small = Subset(trainset_vit, indices_train)\n",
    "\n",
    "trainloader_vit = torch.utils.data.DataLoader(trainset_vit_small, batch_size=BATCH_SIZE, shuffle=True)\n",
    "testloader_vit  = torch.utils.data.DataLoader(testset_vit, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "# Модель CNN\n",
    "cnn = torchvision.models.resnet18(weights=None, num_classes=NUM_CLASSES)\n",
    "cnn.conv1 = nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
    "cnn.to(device)\n",
    "\n",
    "# Модель ViT\n",
    "vit = torchvision.models.vit_b_16(weights='IMAGENET1K_V1')\n",
    "vit.heads.head = nn.Linear(vit.heads.head.in_features, NUM_CLASSES)\n",
    "vit.to(device)\n",
    "\n",
    "\n",
    "# Общая функция обучения\n",
    "def train(model, trainloader, val_loader, epochs):\n",
    "    model.train()\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=LR)\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        for imgs, labels in trainloader:\n",
    "            imgs, labels = imgs.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            out = model(imgs)\n",
    "            loss = criterion(out, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        # Оценка на валидации после каждой эпохи\n",
    "        model.eval()\n",
    "        correct, total = 0, 0\n",
    "        with torch.no_grad():\n",
    "            for imgs, labels in val_loader:\n",
    "                imgs, labels = imgs.to(device), labels.to(device)\n",
    "                out = model(imgs)\n",
    "                preds = out.argmax(1)\n",
    "                correct += (preds == labels).sum().item()\n",
    "                total += labels.size(0)\n",
    "        print(f\"Epoch {epoch+1}/{epochs}: val acc = {correct/total:.4f}\")\n",
    "\n",
    "# Функция подсчёта метрик\n",
    "def eval_model(model, testloader):\n",
    "    model.eval()\n",
    "    acc_metric = MulticlassAccuracy(num_classes=NUM_CLASSES, average='macro').to(device)\n",
    "    f1_metric = MulticlassF1Score(num_classes=NUM_CLASSES, average='macro').to(device)\n",
    "    all_preds, all_labels = [], []\n",
    "    with torch.no_grad():\n",
    "        for imgs, labels in testloader:\n",
    "            imgs, labels = imgs.to(device), labels.to(device)\n",
    "            out = model(imgs)\n",
    "            preds = torch.argmax(out, 1)\n",
    "            acc_metric.update(preds, labels)\n",
    "            f1_metric.update(preds, labels)\n",
    "            all_preds.append(preds.cpu())\n",
    "            all_labels.append(labels.cpu())\n",
    "    acc = acc_metric.compute().item()\n",
    "    f1 = f1_metric.compute().item()\n",
    "    return acc, f1\n",
    "\n",
    "# Обучение и оценка\n",
    "print(\"=== Обучаем ViT ===\")\n",
    "train(vit, trainloader_vit, testloader_vit, EPOCHS)\n",
    "acc_vit, f1_vit = eval_model(vit, testloader_vit)\n",
    "print(f\"ViT_B_16: Test accuracy: {acc_vit:.4f}, Test Macro F1: {f1_vit:.4f}\")\n",
    "\n",
    "\n",
    "print(\"=== Обучаем ResNet (CNN) ===\")\n",
    "train(cnn, trainloader_cnn, testloader_cnn, EPOCHS)\n",
    "acc_cnn, f1_cnn = eval_model(cnn, testloader_cnn)\n",
    "print(f\"ResNet: Test accuracy: {acc_cnn:.4f}, Test Macro F1: {f1_cnn:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.\tУлучшение бейзлайна\n",
    "\n",
    "**a. Формулировка гипотез**\n",
    "\n",
    "Для улучшения базовых моделей воспользуемся следующими гипотезами:\n",
    "\n",
    "1. Аугментации данных:\n",
    "\n",
    "    Использование аугментаций (случайное отражение, случайные повороты) при обучении повысит обобщающую способность моделей.\n",
    "\n",
    "2. Подбор моделей:\n",
    "\n",
    "    Более глубокие или современные архитектуры сверточных нейронных сетей, такие как ResNet34, могут показать лучшие результаты по сравнению с ResNet18.\n",
    "\n",
    "3. Подбор гиперпараметров:\n",
    "\n",
    "    Модификация learning rate и использование scheduler может ускорить сходимость и/или повысить итоговое качество.\n",
    "\n",
    "4. Использование предобученных весов:\n",
    "\n",
    "    Использование предобученных на ImageNet весов для моделей (с последующей донастройкой на Fashion-MNIST) позволит добиться лучшей сходимости, несмотря на разницу в доменах данных.\n",
    "\n",
    "**b. Проверка гипотез**\n",
    "\n",
    "Исходя из проверки гипотез, улучшенный бейзлайн включает:\n",
    "\n",
    "1. Использование аугментаций в обучающей выборке.\n",
    "\n",
    "2. Архитектуры ResNet34 и ViT_B_16 с предобученными весами (transfer learning).\n",
    "\n",
    "3. Learning rate: 5e-4, scheduler на снижение lr при plateu по валидационной accuracy.\n",
    "\n",
    "**d/e. Обучение и оценка на улучшенном бейзлайне**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Улучшенный ViT===\n",
      "Epoch 1: val_acc=0.1967\n",
      "Epoch 2: val_acc=0.3313\n",
      "Epoch 3: val_acc=0.2978\n",
      "ViT_B_16 improved: Test Accuracy = 0.2978, Macro F1 = 0.2708\n",
      "=== Улучшенный ResNet ===\n",
      "Epoch 1: val_acc=0.8443\n",
      "Epoch 2: val_acc=0.8791\n",
      "Epoch 3: val_acc=0.8814\n",
      "ResNet improved: Test Accuracy = 0.8814, Macro F1 = 0.8804\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torchmetrics.classification import MulticlassAccuracy, MulticlassF1Score\n",
    "\n",
    "# Аугментации и трансформации\n",
    "transform_aug_cnn = transforms.Compose([\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomRotation(10),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,))\n",
    "])\n",
    "transform_cnn_test = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,))\n",
    "])\n",
    "\n",
    "transform_aug_vit = transforms.Compose([\n",
    "    transforms.Resize(224),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomRotation(10),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Lambda(lambda x: x.repeat(3, 1, 1)),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "])\n",
    "transform_vit_test = transforms.Compose([\n",
    "    transforms.Resize(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Lambda(lambda x: x.repeat(3, 1, 1)),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "])\n",
    "\n",
    "BATCH_SIZE = 128\n",
    "NUM_CLASSES = 10\n",
    "EPOCHS = 3\n",
    "LR = 5e-4\n",
    "PATIENCE = 2\n",
    "device = torch.device(\"mps\")\n",
    "\n",
    "# Датасеты и DataLoader'ы\n",
    "trainset_cnn = torchvision.datasets.FashionMNIST('data', train=True, download=True, transform=transform_aug_cnn)\n",
    "testset_cnn  = torchvision.datasets.FashionMNIST('data', train=False, download=True, transform=transform_cnn_test)\n",
    "trainset_vit = torchvision.datasets.FashionMNIST('data', train=True, download=True, transform=transform_aug_vit)\n",
    "testset_vit  = torchvision.datasets.FashionMNIST('data', train=False, download=True, transform=transform_vit_test)\n",
    "\n",
    "trainloader_cnn = torch.utils.data.DataLoader(trainset_cnn, batch_size=BATCH_SIZE, shuffle=True)\n",
    "testloader_cnn  = torch.utils.data.DataLoader(testset_cnn, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "N_SUBSET = 5000\n",
    "np.random.seed(42)\n",
    "indices_train = np.random.choice(len(trainset_vit), N_SUBSET, replace=False)\n",
    "trainset_vit_small = Subset(trainset_vit, indices_train)\n",
    "\n",
    "trainloader_vit = torch.utils.data.DataLoader(trainset_vit_small, batch_size=BATCH_SIZE, shuffle=True)\n",
    "testloader_vit  = torch.utils.data.DataLoader(testset_vit, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "\n",
    "# Создание улучшенных моделей\n",
    "\n",
    "# ResNet34 с предобученными весами\n",
    "resnet = torchvision.models.resnet34(weights='IMAGENET1K_V1')\n",
    "resnet.conv1 = nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
    "resnet.fc = nn.Linear(resnet.fc.in_features, NUM_CLASSES)\n",
    "resnet.to(device)\n",
    "\n",
    "# ViT с предобучением\n",
    "vit = torchvision.models.vit_b_16(weights='IMAGENET1K_V1')\n",
    "vit.heads.head = nn.Linear(vit.heads.head.in_features, NUM_CLASSES)\n",
    "vit.to(device)\n",
    "\n",
    "# Функции для обучения и оценки\n",
    "def train(model, trainloader, valloader, epochs):\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=LR)\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'max', patience=PATIENCE, factor=0.5)\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        for imgs, labels in trainloader:\n",
    "            imgs, labels = imgs.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            out = model(imgs)\n",
    "            loss = criterion(out, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        model.eval()\n",
    "        correct, total = 0, 0\n",
    "        with torch.no_grad():\n",
    "            for imgs, labels in valloader:\n",
    "                imgs, labels = imgs.to(device), labels.to(device)\n",
    "                preds = model(imgs).argmax(1)\n",
    "                correct += (preds == labels).sum().item()\n",
    "                total += labels.size(0)\n",
    "        val_acc = correct/total\n",
    "        print(f'Epoch {epoch+1}: val_acc={val_acc:.4f}')\n",
    "        scheduler.step(val_acc)\n",
    "\n",
    "def eval_model(model, testloader):\n",
    "    model.eval()\n",
    "    acc_metric = MulticlassAccuracy(num_classes=NUM_CLASSES, average='macro').to(device)\n",
    "    f1_metric = MulticlassF1Score(num_classes=NUM_CLASSES, average='macro').to(device)\n",
    "    with torch.no_grad():\n",
    "        for imgs, labels in testloader:\n",
    "            imgs, labels = imgs.to(device), labels.to(device)\n",
    "            preds = model(imgs).argmax(1)\n",
    "            acc_metric.update(preds, labels)\n",
    "            f1_metric.update(preds, labels)\n",
    "    acc = acc_metric.compute().item()\n",
    "    f1 = f1_metric.compute().item()\n",
    "    return acc, f1\n",
    "\n",
    "\n",
    "# Обучение улучшенных моделей и вывод метрик\n",
    "print('=== Улучшенный ViT===')\n",
    "train(vit, trainloader_vit, testloader_vit, EPOCHS)\n",
    "acc_vit, f1_vit = eval_model(vit, testloader_vit)\n",
    "print(f'ViT_B_16 improved: Test Accuracy = {acc_vit:.4f}, Macro F1 = {f1_vit:.4f}')\n",
    "\n",
    "print('=== Улучшенный ResNet ===')\n",
    "train(resnet, trainloader_cnn, testloader_cnn, EPOCHS)\n",
    "acc_rn, f1_rn = eval_model(resnet, testloader_cnn)\n",
    "print(f'ResNet improved: Test Accuracy = {acc_rn:.4f}, Macro F1 = {f1_rn:.4f}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**g. Выводы**\n",
    "\n",
    "1. ResNet правильно реагирует на улучшения: аугментации, увеличение глубины, transfer learning и scheduler даже на таком деперсонализированном датасете как Fashion-MNIST дают хороший, ожидаемый прирост.\n",
    "\n",
    "2. ViT заметная деградация качества. Возможно сказался высокий LR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Имплементация алгоритма машинного обучения\n",
    "\n",
    "**a. Самостоятельно имплементировать модели машинного обучения**\n",
    "Для сравнения с нейронными сетями рассмотрим:\n",
    "\n",
    "1. Логистическую регрессию (multinomial)\n",
    "\n",
    "2. Метод опорных векторов (SVM)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**b/c. Обучение моделей на Fashion-MNIST и оценка качества моделей**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "import numpy as np\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "\n",
    "# Загружаем dataloader с нормализацией как для нейросети\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,))\n",
    "])\n",
    "\n",
    "trainset = torchvision.datasets.FashionMNIST('data', train=True, download=True, transform=transform)\n",
    "testset  = torchvision.datasets.FashionMNIST('data', train=False, download=True, transform=transform)\n",
    "\n",
    "# Преобразуем в numpy\n",
    "X_train = trainset.data.numpy().reshape(-1, 28*28) / 255.0   # нормируем [0,1]\n",
    "y_train = trainset.targets.numpy()\n",
    "X_test  = testset.data.numpy().reshape(-1, 28*28) / 255.0\n",
    "y_test  = testset.targets.numpy()\n",
    "\n",
    "N_SUB = 5000\n",
    "X_train_small = X_train[:N_SUB]\n",
    "y_train_small = y_train[:N_SUB]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Логистическая регрессия:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rudadadadada/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Логистическая регрессия: accuracy = 0.8112, macro F1 = 0.8118\n"
     ]
    }
   ],
   "source": [
    "logreg = LogisticRegression(max_iter=1000, multi_class='multinomial', solver='lbfgs')\n",
    "logreg.fit(X_train_small, y_train_small)\n",
    "y_pred_logreg = logreg.predict(X_test)\n",
    "\n",
    "acc_logreg = accuracy_score(y_test, y_pred_logreg)\n",
    "f1_logreg = f1_score(y_test, y_pred_logreg, average='macro')\n",
    "print(f\"Логистическая регрессия: accuracy = {acc_logreg:.4f}, macro F1 = {f1_logreg:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**SVM:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM (Linear): accuracy = 0.7867, macro F1 = 0.7867\n"
     ]
    }
   ],
   "source": [
    "svc = LinearSVC(max_iter=2000)\n",
    "svc.fit(X_train_small, y_train_small)\n",
    "y_pred_svc = svc.predict(X_test)\n",
    "\n",
    "acc_svc = accuracy_score(y_test, y_pred_svc)\n",
    "f1_svc = f1_score(y_test, y_pred_svc, average='macro')\n",
    "print(f\"SVM (Linear): accuracy = {acc_svc:.4f}, macro F1 = {f1_svc:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**e. Выводы**\n",
    "\n",
    "1. Классические модели (логистическая регрессия, SVM) на Fashion-MNIST показывают качество на уровне 78-81% (accuracy), что весьма достойно для простых моделей без ручных признаков.\n",
    "\n",
    "2. Глубокие нейронные сети (ResNet18, ViT_B_16) демонстрируют лучшие результаты (accuracy ≈ 87–90%), особенно при большем обучающем датасете и наличии аугментаций.\n",
    "\n",
    "3. Разрыв в качестве объясняется тем, что нейронные сети могут извлекать более сложные абстрактные признаки, а классические модели работают только с \"сырыми\" пикселями.\n",
    "\n",
    "4. Классические модели гораздо быстрее обучаются на CPU на небольшом количестве данных и не требуют GPU."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**f/g/h. Добавить техники из улучшенного бейзлайна и обучить модели, оценка качества моделей**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import torchvision\n",
    "\n",
    "trainset = torchvision.datasets.FashionMNIST('data', train=True, download=True)\n",
    "testset  = torchvision.datasets.FashionMNIST('data', train=False, download=True)\n",
    "\n",
    "X_train = trainset.data.numpy().reshape(-1, 28*28).astype(np.float32) / 255.0\n",
    "y_train = trainset.targets.numpy()\n",
    "X_test  = testset.data.numpy().reshape(-1, 28*28).astype(np.float32) / 255.0\n",
    "y_test  = testset.targets.numpy()\n",
    "\n",
    "# Стандартизация\n",
    "scaler = StandardScaler()\n",
    "X_train_std = scaler.fit_transform(X_train)\n",
    "X_test_std = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Логистическая регрессия:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rudadadadada/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
      "  warnings.warn(\n",
      "/Users/rudadadadada/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
      "  warnings.warn(\n",
      "/Users/rudadadadada/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
      "  warnings.warn(\n",
      "/Users/rudadadadada/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
      "  warnings.warn(\n",
      "/Users/rudadadadada/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
      "  warnings.warn(\n",
      "/Users/rudadadadada/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
      "  warnings.warn(\n",
      "/Users/rudadadadada/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
      "  warnings.warn(\n",
      "/Users/rudadadadada/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
      "  warnings.warn(\n",
      "/Users/rudadadadada/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
      "  warnings.warn(\n",
      "/Users/rudadadadada/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Лучшие параметры логрегрессии: {'C': 0.1}\n",
      "Logistic Regression improved: accuracy = 0.8433, macro F1 = 0.8425\n"
     ]
    }
   ],
   "source": [
    "# 3. GridSearch по C для логрегрессии\n",
    "param_grid_logreg = {'C': [0.1, 1, 3]}\n",
    "logreg = LogisticRegression(max_iter=2000, multi_class='multinomial', solver='lbfgs')\n",
    "gs_logreg = GridSearchCV(logreg, param_grid_logreg, cv=3, n_jobs=1)\n",
    "gs_logreg.fit(X_train_std, y_train)\n",
    "print(\"Лучшие параметры логрегрессии:\", gs_logreg.best_params_)\n",
    "best_logreg = gs_logreg.best_estimator_\n",
    "\n",
    "y_pred_logreg = best_logreg.predict(X_test_std)\n",
    "acc_logreg = accuracy_score(y_test, y_pred_logreg)\n",
    "f1_logreg = f1_score(y_test, y_pred_logreg, average='macro')\n",
    "print(f\"Logistic Regression improved: accuracy = {acc_logreg:.4f}, macro F1 = {f1_logreg:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**SVM:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rudadadadada/Library/Python/3.9/lib/python/site-packages/sklearn/svm/_base.py:1249: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/Users/rudadadadada/Library/Python/3.9/lib/python/site-packages/sklearn/svm/_base.py:1249: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Лучшие параметры SVM: {'C': 1}\n",
      "SVM improved: accuracy = 0.7450, macro F1 = 0.7454\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rudadadadada/Library/Python/3.9/lib/python/site-packages/sklearn/svm/_base.py:1249: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# 4. GridSearch по C для SVM (LinearSVC)\n",
    "param_grid_svc = {'C': [1]}\n",
    "svc = LinearSVC(max_iter=1000)\n",
    "gs_svc = GridSearchCV(svc, param_grid_svc, cv=2, n_jobs=1)\n",
    "gs_svc.fit(X_train_std[:5000], y_train[:5000])\n",
    "print(\"Лучшие параметры SVM:\", gs_svc.best_params_)\n",
    "best_svc = gs_svc.best_estimator_\n",
    "\n",
    "y_pred_svc = best_svc.predict(X_test_std)\n",
    "acc_svc = accuracy_score(y_test, y_pred_svc)\n",
    "f1_svc = f1_score(y_test, y_pred_svc, average='macro')\n",
    "print(f\"SVM improved: accuracy = {acc_svc:.4f}, macro F1 = {f1_svc:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**j. Выводы**\n",
    "\n",
    "1. Стандартизация признаков и подбор C через GridSearchCV позволили повысить качество логистической регресии на 3%, а качество SVM ухудшилось на столько же.\n",
    "\n",
    "2. Улучшенные логистическая регрессия и SVM минимально сократили разрыв по метрикам с простейшими сверточными сетями, но нейросеть ResNet всё равно опережает их на 5%. Улучшенный ViT демонстрирует худшее качество (возможно, из-за высокого LR)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Итог:\n",
    "\n",
    "| Модель            | Baseline Accuracy | Baseline F1 | Improved Accuracy | Improved F1 |\n",
    "|-------------------|------------------|-------------|-------------------|-------------|\n",
    "| ViT               | 0.9003           | 0.9016      | 0.2978            | 0.2708      |\n",
    "| ResNet            | 0.8759           | 0.8757      | 0.8814            | 0.8804      |\n",
    "| Лог. регрессия    | 0.8112           | 0.8118      | 0.8433            | 0.8425      |\n",
    "| SVM               | 0.7867           | 0.7867      | 0.7450            | 0.7454      |"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
