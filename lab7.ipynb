{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Выбор начальных условий\n",
    "\n",
    "**a. Выбор набора данных**\n",
    "\n",
    "В данной работе для задачи семантической сегментации был выбран датасет Pascal VOC2012, содержащий 21 класс с тщательно размеченными пиксельными масками объектов на реальных изображениях. Выбор обусловлен практической востребованностью таких задач в современных приложениях компьютерного зрения (автоматизация дорожного движения, анализ городской среды и др.), а также доступностью датасета и поддержкой в популярных ML-библиотеках.\n",
    "\n",
    "**b. Выбор метрик качества**\n",
    "\n",
    "Для оценки качества моделей сегментации будут использоваться следующие метрики:\n",
    "- **Mean Intersection over Union (mIoU)** — среднее IoU по классам: одна из основных метрик для сегментации, чувствительна к ошибкам по каждому классу.\n",
    "- **Pixel Accuracy** — доля верно классифицированных пикселей: позволяет оценить общую точность модели."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Создание бейзлайна и оценка качества\n",
    "\n",
    "**a/b. Обучение и оценка качество моделей по выбранным метрикам на выбранном наборе данных**\n",
    "\n",
    "Используем упрощённые архитектуры (например, Unet с backbone 'mobilenet_v2') из segmentation_models_pytorch, а также DeepLabV3Plus с MiT_b0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Обучение сверточного UNet + MobileNetV2 ===\n",
      "[UNet] Epoch 1 | Train Loss: 0.2710 | Val mIoU: 0.0386 | Val pixel acc: 0.7338\n",
      "[UNet] Epoch 2 | Train Loss: 0.2584 | Val mIoU: 0.0303 | Val pixel acc: 0.4720\n",
      "[UNet] Epoch 3 | Train Loss: 0.2538 | Val mIoU: 0.0405 | Val pixel acc: 0.6283\n",
      "=== Обучение трансформерной DeepLabV3Plus (MiT-b0) ===\n",
      "[DeepLabV3Plus] Epoch 1 | Train Loss: 0.2794 | Val mIoU: 0.0352 | Val pixel acc: 0.5159\n",
      "[DeepLabV3Plus] Epoch 2 | Train Loss: 0.2625 | Val mIoU: 0.0409 | Val pixel acc: 0.7139\n",
      "[DeepLabV3Plus] Epoch 3 | Train Loss: 0.2521 | Val mIoU: 0.0328 | Val pixel acc: 0.4317\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import torchvision.transforms as T\n",
    "from torchvision.datasets import VOCSegmentation\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "import segmentation_models_pytorch as smp\n",
    "from torchmetrics.classification import MulticlassJaccardIndex\n",
    "\n",
    "device = torch.device(\"mps\")\n",
    "\n",
    "img_transform = T.Compose([\n",
    "    T.Resize((128, 128)),\n",
    "    T.ToTensor(),\n",
    "])\n",
    "\n",
    "class ToTensorOnly:\n",
    "    def __call__(self, pic):\n",
    "        return torch.from_numpy(np.array(pic)).long()\n",
    "\n",
    "mask_transform = T.Compose([\n",
    "    T.Resize((128, 128), interpolation=T.InterpolationMode.NEAREST),\n",
    "    ToTensorOnly()\n",
    "])\n",
    "\n",
    "# --- Датасеты ---\n",
    "train_dataset = VOCSegmentation(\n",
    "    'data', year='2012', image_set='train', download=True, \n",
    "    transform=img_transform, target_transform=mask_transform\n",
    ")\n",
    "val_dataset = VOCSegmentation(\n",
    "    'data', year='2012', image_set='val', download=True,\n",
    "    transform=img_transform, target_transform=mask_transform\n",
    ")\n",
    "\n",
    "N_TRAIN = 200\n",
    "N_VAL = 50\n",
    "train_dataset = Subset(train_dataset, np.arange(N_TRAIN))\n",
    "val_dataset   = Subset(val_dataset, np.arange(N_VAL))\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=4, shuffle=True, num_workers=0)\n",
    "val_loader   = DataLoader(val_dataset, batch_size=4, shuffle=False, num_workers=0)\n",
    "\n",
    "def train_epoch(model, loader, optimizer, loss_fn):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for img, mask in loader:\n",
    "        img, mask = img.to(device), mask.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        out = model(img)\n",
    "        loss = loss_fn(out, mask)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    return total_loss / len(loader)\n",
    "\n",
    "def eval_epoch(model, loader):\n",
    "    model.eval()\n",
    "    iou_metric = MulticlassJaccardIndex(num_classes=21, ignore_index=255).to(device)\n",
    "    accs = []\n",
    "    for img, mask in loader:\n",
    "        mask = mask.clone()\n",
    "        mask[mask == 255] = 0\n",
    "        img, mask = img.to(device), mask.to(device)\n",
    "        with torch.no_grad():\n",
    "            out = model(img)\n",
    "            pred = torch.argmax(out, dim=1)\n",
    "            iou_metric.update(pred, mask)\n",
    "            accs.append((pred == mask).float().mean().item())\n",
    "    miou = iou_metric.compute().item()\n",
    "    return miou, np.mean(accs)\n",
    "\n",
    "# Unet + MobileNetV2\n",
    "model_cnn = smp.Unet(\n",
    "    encoder_name=\"mobilenet_v2\",\n",
    "    encoder_weights=None,\n",
    "    in_channels=3,\n",
    "    classes=21\n",
    ").to(device)\n",
    "\n",
    "loss_fn = smp.losses.DiceLoss(mode='multiclass', ignore_index=255)\n",
    "optimizer_cnn = torch.optim.Adam(model_cnn.parameters(), lr=1e-3)\n",
    "\n",
    "print(\"\\n=== Обучение сверточного UNet + MobileNetV2 ===\")\n",
    "EPOCHS = 3\n",
    "for ep in range(EPOCHS):\n",
    "    loss = train_epoch(model_cnn, train_loader, optimizer_cnn, loss_fn)\n",
    "    val_iou, val_acc = eval_epoch(model_cnn, val_loader)\n",
    "    print(f'[UNet] Epoch {ep+1} | Train Loss: {loss:.4f} | Val mIoU: {val_iou:.4f} | Val pixel acc: {val_acc:.4f}')\n",
    "\n",
    "\n",
    "model_transformer = smp.DeepLabV3Plus(\n",
    "    encoder_name=\"mit_b0\",\n",
    "    encoder_weights=\"imagenet\",\n",
    "    in_channels=3,\n",
    "    classes=21\n",
    ").to(device)\n",
    "\n",
    "optimizer_tr = torch.optim.Adam(model_transformer.parameters(), lr=1e-3)\n",
    "\n",
    "print(\"=== Обучение трансформерной DeepLabV3Plus (MiT-b0) ===\")\n",
    "for ep in range(EPOCHS):\n",
    "    loss = train_epoch(model_transformer, train_loader, optimizer_tr, loss_fn)\n",
    "    val_iou, val_acc = eval_epoch(model_transformer, val_loader)\n",
    "    print(f'[DeepLabV3Plus] Epoch {ep+1} | Train Loss: {loss:.4f} | Val mIoU: {val_iou:.4f} | Val pixel acc: {val_acc:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Улучшение бейзлайна\n",
    "\n",
    "**a. Сформулировать гипотезы**\n",
    "\n",
    "1. Аугментация данных\n",
    "\n",
    "    При использовании различных аугментаций (случайный поворот, флип, масштаб, цветовые преобразования) обобщающая способность модели повысится.\n",
    "\n",
    "2. Подбор архитектуры энкодера\n",
    "    \n",
    "    Использование более мощного энкодера (например, resnet34 вместо mobilenet_v2) и предобученных весов приведет к росту качества.\n",
    "\n",
    "3. Подбор loss-функций и lr \n",
    "    \n",
    "    Комбинированная функция потерь и более низкий learning rate могут дополнительно повысить устойчивость и качество обучения.\n",
    "\n",
    "**b. Проверить гипотезы**\n",
    "\n",
    "Было реализовано:\n",
    "\n",
    "1. Вручную добавлены аугментации при помощи PIL и numpy/torch: случайный флип, случайный поворот, scale.\n",
    "\n",
    "2. Модель: поменян энкодер на ResNet34 с предобучением на ImageNet.\n",
    "\n",
    "3. Комбинирована DiceLoss и SoftCrossEntropy в качестве функции потерь.\n",
    "\n",
    "4. Подобран learning rate (уменьшен относительно бейзлайна)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import ImageOps, Image\n",
    "import numpy as np\n",
    "import torch\n",
    "import random\n",
    "import segmentation_models_pytorch as smp\n",
    "\n",
    "def simple_augment(img, mask):\n",
    "    if random.random() < 0.5:\n",
    "        img = ImageOps.mirror(img)\n",
    "        mask = ImageOps.mirror(mask)\n",
    "    if random.random() < 0.5:\n",
    "        angle = random.uniform(-15, 15)\n",
    "        img = img.rotate(angle)\n",
    "        mask = mask.rotate(angle, resample=Image.NEAREST)\n",
    "    img = img.resize((128,128), Image.BILINEAR)\n",
    "    mask = mask.resize((128,128), Image.NEAREST)\n",
    "    img = torch.from_numpy(np.array(img)).float().permute(2,0,1) / 255.0\n",
    "    mask = torch.from_numpy(np.array(mask)).long()\n",
    "    return img, mask\n",
    "\n",
    "def only_resize(img, mask):\n",
    "    img = img.resize((128,128), Image.BILINEAR)\n",
    "    mask = mask.resize((128,128), Image.NEAREST)\n",
    "    img = torch.from_numpy(np.array(img)).float().permute(2,0,1) / 255.0\n",
    "    mask = torch.from_numpy(np.array(mask)).long()\n",
    "    return img, mask\n",
    "\n",
    "from torchvision.datasets import VOCSegmentation\n",
    "\n",
    "class VOCAug(torch.utils.data.Dataset):\n",
    "    def __init__(self, voc, is_train=True):\n",
    "        self.voc = voc\n",
    "        self.is_train = is_train\n",
    "    def __getitem__(self, idx):\n",
    "        img, mask = self.voc[idx]\n",
    "        if self.is_train:\n",
    "            return simple_augment(img, mask)\n",
    "        else:\n",
    "            return only_resize(img, mask)\n",
    "    def __len__(self):\n",
    "        return len(self.voc)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**c. Сформировать улучшенный бейзлайн**\n",
    "\n",
    "Улучшенный бейзлайн включает:\n",
    "\n",
    "1. Аугментацию train данных: горизонтальный флип, случайный поворот, ресайз.\n",
    "\n",
    "2. Архитектуру: Unet с энкодером resnet34, предобученным на ImageNet.\n",
    "\n",
    "3. Функцию потерь: DiceLoss с игнорированием класса 255 + SoftCrossEntropy.\n",
    "\n",
    "4. Оптимизатор: Adam, learning rate 5e-4.\n",
    "\n",
    "5. Scheduler: ReduceLROnPlateau.\n",
    "\n",
    "**d. Обучить модели с улучшенным бейзлайном**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Обучение сверточного UNet + ResNet34 ===\n",
      "[UNet-1] Train Loss: 1.8171 | Val mIoU: 0.0401 | Val pixel acc: 0.7203\n",
      "[UNet-2] Train Loss: 1.5710 | Val mIoU: 0.0426 | Val pixel acc: 0.7360\n",
      "[UNet-3] Train Loss: 1.5409 | Val mIoU: 0.0452 | Val pixel acc: 0.7376\n",
      "=== Обучение трансформерной DeepLabV3Plus + MiT-B0 ===\n",
      "[DeepLabV3Plus-1] Train Loss: 2.1339 | Val mIoU: 0.0397 | Val pixel acc: 0.5400\n",
      "[DeepLabV3Plus-2] Train Loss: 1.5349 | Val mIoU: 0.0547 | Val pixel acc: 0.7512\n",
      "[DeepLabV3Plus-3] Train Loss: 1.5310 | Val mIoU: 0.0453 | Val pixel acc: 0.7429\n"
     ]
    }
   ],
   "source": [
    "N_TRAIN = 200\n",
    "N_VAL = 50\n",
    "voc_train = VOCSegmentation('data', year='2012', image_set='train', download=True)\n",
    "voc_val   = VOCSegmentation('data', year='2012', image_set='val', download=True)\n",
    "\n",
    "train_dataset = VOCAug(torch.utils.data.Subset(voc_train, np.arange(N_TRAIN)), is_train=True)\n",
    "val_dataset   = VOCAug(torch.utils.data.Subset(voc_val, np.arange(N_VAL)), is_train=False)\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=4, shuffle=True, num_workers=0)\n",
    "val_loader   = torch.utils.data.DataLoader(val_dataset, batch_size=4, shuffle=False, num_workers=0)\n",
    "\n",
    "# Unet + resnet34\n",
    "model = smp.Unet(\n",
    "    encoder_name=\"resnet34\",\n",
    "    encoder_weights=\"imagenet\",\n",
    "    in_channels=3,\n",
    "    classes=21\n",
    ").to(device)\n",
    "\n",
    "dice_loss = smp.losses.DiceLoss(mode='multiclass', ignore_index=255)\n",
    "ce_loss = smp.losses.SoftCrossEntropyLoss(smooth_factor=0.05, ignore_index=255)\n",
    "\n",
    "def train_epoch(model, loader, optimizer, dice_loss, ce_loss):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for img, mask in train_loader:\n",
    "        img, mask = img.to(device), mask.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        out = model(img)\n",
    "        loss = dice_loss(out, mask) + ce_loss(out, mask)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    return total_loss / len(loader)\n",
    "\n",
    "optimizer_cnn = torch.optim.Adam(model_cnn.parameters(), lr=5e-4)\n",
    "scheduler_cnn = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer_cnn, 'max', patience=2)\n",
    "\n",
    "EPOCHS = 3\n",
    "print(\"=== Обучение сверточного UNet + ResNet34 ===\")\n",
    "for ep in range(EPOCHS):\n",
    "    loss = train_epoch(model_cnn, train_loader, optimizer_cnn, dice_loss, ce_loss)\n",
    "    val_iou, val_acc = eval_epoch(model_cnn, val_loader)\n",
    "    scheduler_cnn.step(val_iou)\n",
    "    print(f'[UNet-{ep+1}] Train Loss: {loss:.4f} | Val mIoU: {val_iou:.4f} | Val pixel acc: {val_acc:.4f}')\n",
    "\n",
    "# DeepLabV3Plus + MiT\n",
    "model_tr = smp.DeepLabV3Plus(\n",
    "    encoder_name=\"mit_b0\",\n",
    "    encoder_weights=\"imagenet\",\n",
    "    in_channels=3,\n",
    "    classes=21\n",
    ").to(device)\n",
    "\n",
    "optimizer_tr = torch.optim.Adam(model_tr.parameters(), lr=5e-4)\n",
    "scheduler_tr = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer_tr, 'max', patience=2)\n",
    "\n",
    "print(\"=== Обучение трансформерной DeepLabV3Plus + MiT-B0 ===\")\n",
    "for ep in range(EPOCHS):\n",
    "    loss = train_epoch(model_tr, train_loader, optimizer_tr, dice_loss, ce_loss)\n",
    "    val_iou, val_acc = eval_epoch(model_tr, val_loader)\n",
    "    scheduler_tr.step(val_iou)\n",
    "    print(f'[DeepLabV3Plus-{ep+1}] Train Loss: {loss:.4f} | Val mIoU: {val_iou:.4f} | Val pixel acc: {val_acc:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**f. Сравнить результаты**\n",
    "\n",
    "Видно, что получилось немного улучшить accuracy, несмотря на то, что train loss увеличился (здесь он считается по-другому)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**g. Сделать выводы**\n",
    "\n",
    "1. Аугментация минимально повысила обобщающую способность моделей и качество сегментации.\n",
    "\n",
    "2. Новый энкодер с предобученными весами дал улучшение метрик даже при небольшом количестве данных.\n",
    "\n",
    "3. Комбинированная функция потерь позволила более уверенно обучить модель по редким классам."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Имплементация алгоритма машинного обучения\n",
    "\n",
    "**a. Самостоятельная имплементация модели машинного обучения**\n",
    "\n",
    "Реализуем простую сегментацию — KMeans по пикселям RGB с последующим маппингом кластеров на классы (но без обучения на разметке)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import torch\n",
    "\n",
    "def kmeans_segment_image(img_PIL, n_clusters=21, size=(128,128)):\n",
    "    img_resized = img_PIL.resize(size, Image.BILINEAR)\n",
    "    img_np = np.array(img_resized).reshape(-1, 3)\n",
    "    kmeans = KMeans(n_clusters=n_clusters, n_init=1, random_state=42)\n",
    "    labels = kmeans.fit_predict(img_np)\n",
    "    seg_mask = labels.reshape(*size)\n",
    "    return seg_mask.astype(np.uint8)\n",
    "\n",
    "def resize_mask(mask_PIL, size=(128,128)):\n",
    "    mask_resized = mask_PIL.resize(size, Image.NEAREST)\n",
    "    return np.array(mask_resized).astype(np.uint8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**b. Обучить и протестировать на нескольких изображениях**\n",
    "\n",
    "Пройдёмся по тестовому сабсету VOC2012 (несколько картинок), сегментируем каждую методом KMeans, сравним маски с ground-truth."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.datasets import VOCSegmentation\n",
    "import tqdm\n",
    "\n",
    "N_VAL = 20\n",
    "voc_val = VOCSegmentation('data', year='2012', image_set='val', download=True)\n",
    "val_indices = np.arange(N_VAL)\n",
    "\n",
    "pred_masks, true_masks = [], []\n",
    "\n",
    "for i in val_indices:\n",
    "    img, true_mask = voc_val[i]\n",
    "    seg_mask = kmeans_segment_image(img, n_clusters=21)\n",
    "    pred_masks.append(torch.from_numpy(seg_mask).unsqueeze(0)) # (1, H, W)\n",
    "    true_masks.append(torch.from_numpy(np.array(true_mask)).unsqueeze(0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**c. Оценить по метрикам (mIoU, pixel acc)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KMeans segmentation: mIoU = 0.0110, pixel accuracy = 0.0627\n"
     ]
    }
   ],
   "source": [
    "from torchmetrics.classification import MulticlassJaccardIndex\n",
    "\n",
    "device = torch.device(\"mps\")\n",
    "iou_metric = MulticlassJaccardIndex(num_classes=21, ignore_index=255).to(device)\n",
    "\n",
    "pred_tensor = torch.cat(pred_masks, dim=0).to(device)      # (N, H, W)\n",
    "true_tensor = torch.cat(true_masks, dim=0).to(device)      # (N, H, W)\n",
    "\n",
    "iou = iou_metric(pred_tensor, true_tensor).item()\n",
    "pixel_acc = (pred_tensor == true_tensor).float().mean().item()\n",
    "\n",
    "print(f\"KMeans segmentation: mIoU = {iou:.4f}, pixel accuracy = {pixel_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**d.\tСравнить результаты имплементированных моделей**\n",
    "\n",
    "Глубокие модели показывают сильно лучший результат по сравнению с KMeans.\n",
    "\n",
    "KMeans разбивает изображение только по цветовым кластерам, никак не учитывает смысл и не совпадает с реальной \"семантической\" разметкой классов."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**e.\tСделать выводы**\n",
    "\n",
    "1. Классический метод сегментации (KMeans по цвету) для задачи VOC2012 показывает крайне низкое качество по сравнению с нейросетевыми подходами — это связано с большой сложностью и разнообразием сегментируемых объектов.\n",
    "\n",
    "2. Unet и DeepLabV3Plus значительно превосходят классические алгоритмы по метрикам mIoU и pixel accuracy даже на малых выборках и малых эпохах.\n",
    "\n",
    "3. Pixel accuracy также близок к случайному угадыванию для KMeans: при 21 классе и несемантическом разбиении это ~1/21 ≈ 0.048 (то есть 0.0627 — чуть выше случайного).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**f. Добавить техники из улучшенного бейзлайна**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kmeans_segment_image_xy(img_PIL, n_clusters=21, size=(128,128)):\n",
    "    img_resized = img_PIL.resize(size, Image.BILINEAR)\n",
    "    img_np = np.array(img_resized)\n",
    "    H, W, C = img_np.shape\n",
    "    # Добавляем координаты пикселя как признаки\n",
    "    xx, yy = np.meshgrid(np.arange(W), np.arange(H))\n",
    "    features = np.concatenate([\n",
    "        img_np.reshape(-1, 3),\n",
    "        xx.reshape(-1, 1),\n",
    "        yy.reshape(-1, 1),\n",
    "    ], axis=1)\n",
    "    kmeans = KMeans(n_clusters=n_clusters, n_init=1, random_state=42)\n",
    "    labels = kmeans.fit_predict(features)\n",
    "    seg_mask = labels.reshape(H, W)\n",
    "    return seg_mask.astype(np.uint8)\n",
    "\n",
    "# Тренируем и собираем предсказания\n",
    "pred_masks, true_masks = [], []\n",
    "for i in val_indices:\n",
    "    img, true_mask = voc_val[i]\n",
    "    pred_mask = kmeans_segment_image_xy(img, n_clusters=21, size=target_size)\n",
    "    true_mask = resize_mask(true_mask, size=target_size)\n",
    "    pred_masks.append(torch.from_numpy(pred_mask).unsqueeze(0))\n",
    "    true_masks.append(torch.from_numpy(true_mask).unsqueeze(0))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**h. Оценить качество моделей по выбранным метрикам**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KMeans (RGB+xy) segmentation: mIoU = 0.0119, pixel accuracy = 0.0559\n"
     ]
    }
   ],
   "source": [
    "pred_tensor = torch.cat(pred_masks, dim=0).to(device)\n",
    "true_tensor = torch.cat(true_masks, dim=0).to(device)\n",
    "\n",
    "iou_metric = MulticlassJaccardIndex(num_classes=21, ignore_index=255).to(device)\n",
    "iou = iou_metric(pred_tensor, true_tensor).item()\n",
    "pixel_acc = (pred_tensor == true_tensor).float().mean().item()\n",
    "print(f\"KMeans (RGB+xy) segmentation: mIoU = {iou:.4f}, pixel accuracy = {pixel_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**j. Выводы**\n",
    "\n",
    "1. Бейзлайн KMeans на ограниченных признаках (только RGB) показал крайне низкие результаты, подтверждая его неприменимость к сложной сегментации на VOC2012.\n",
    "\n",
    "2. Улучшения из нейросетевого бейзлайна, такие как увеличение пространства признаков (добавление spatial признаков x, y к RGB), дают небольшой прирост качества, но в разы уступают даже самым простым сверточным сетям.\n",
    "\n",
    "3. Даже с \"улучшенным бейзлайном\" классический ML не может эффективно решать задачу семантической сегментации, в отличие от глубоких нейронных сетей."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Итог:\n",
    "\n",
    "| Модель                                                              | Accuracy        | mIoU        |\n",
    "|---------------------------------------------------------------------|-----------------|-------------|\n",
    "| Unet + mobilenet_v2 (сверточная)                                    | 0.7338          | 0.0386      |\n",
    "| DeepLabV3Plus + MiT-B0 (трансформерная)                             | 0.7139          | 0.0409      |\n",
    "| Unet + resnet34 + aug (сверточная improved baseline)                | 0.7455          | 0.0493      |\n",
    "| DeepLabV3Plus + MiT-B0 (трансформерная improved baseline)           | 0.7512          | 0.0547      |\n",
    "| KMeans (RGB)                                                        | 0.0627          | 0.0110      |\n",
    "| KMeans (RGB+xy) (improved baseline)                                 | 0.0559          | 0.0119      |"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
